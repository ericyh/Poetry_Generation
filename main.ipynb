{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9686\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "corpus = []\n",
    "\n",
    "def read_text_file(file_path): \n",
    "    with open(file_path, 'r') as f: \n",
    "        file = f.read()\n",
    "        for line in file.split(\"\\n\"):\n",
    "            corpus.append(line)\n",
    "for file in os.listdir(): \n",
    "    file_path = f\"{file}\"\n",
    "    read_text_file(file_path) \n",
    "\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5819\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    3   41   10    2\n",
      "   614    4    5]\n",
      " [   0    0    0    0    0    0    0    0    0    3   41   10    2  614\n",
      "     4    5 1940]\n",
      " [   0    0    0    0    0    0    0    0    3   41   10    2  614    4\n",
      "     5 1940  408]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0  615    5]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   615    5  266]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0  615\n",
      "     5  266    3]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0  615    5\n",
      "   266    3  335]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  615    5  266\n",
      "     3  335    5]\n",
      " [   0    0    0    0    0    0    0    0    0    0  615    5  266    3\n",
      "   335    5  336]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    3 1143]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     3 1143   46]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    3\n",
      "  1143   46   22]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    3 1143\n",
      "    46   22    7]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    3 1143   46\n",
      "    22    7   66]\n",
      " [   0    0    0    0    0    0    0    0    0    0    3 1143   46   22\n",
      "     7   66  296]\n",
      " [   0    0    0    0    0    0    0    0    0    3 1143   46   22    7\n",
      "    66  296    3]\n",
      " [   0    0    0    0    0    0    0    0    3 1143   46   22    7   66\n",
      "   296    3 1941]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0   67  112]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    67  112   13]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0   67\n",
      "   112   13  204]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0   67  112\n",
      "    13  204   14]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0   67  112   13\n",
      "   204   14   13]\n",
      " [   0    0    0    0    0    0    0    0    0    0   67  112   13  204\n",
      "    14   13 1942]\n",
      " [   0    0    0    0    0    0    0    0    0   67  112   13  204   14\n",
      "    13 1942    7]\n",
      " [   0    0    0    0    0    0    0    0   67  112   13  204   14   13\n",
      "  1942    7 1144]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0  157  219]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   157  219    6]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0  157\n",
      "   219    6 1943]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0  157  219\n",
      "     6 1943   39]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  157  219    6\n",
      "  1943   39    5]\n",
      " [   0    0    0    0    0    0    0    0    0    0  157  219    6 1943\n",
      "    39    5  204]\n",
      " [   0    0    0    0    0    0    0    0    0  157  219    6 1943   39\n",
      "     5  204 1944]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    3  181]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     3  181    2]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    3\n",
      "   181    2  864]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    3  181\n",
      "     2  864   46]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    3  181    2\n",
      "   864   46 1945]\n",
      " [   0    0    0    0    0    0    0    0    0    0    3  181    2  864\n",
      "    46 1945    3]\n",
      " [   0    0    0    0    0    0    0    0    0    3  181    2  864   46\n",
      "  1945    3 1145]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0  157   42]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   157   42 1946]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0  157\n",
      "    42 1946 1947]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0  157   42\n",
      "  1946 1947    8]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  157   42 1946\n",
      "  1947    8   51]\n",
      " [   0    0    0    0    0    0    0    0    0    0  157   42 1946 1947\n",
      "     8   51 1948]\n",
      " [   0    0    0    0    0    0    0    0    0  157   42 1946 1947    8\n",
      "    51 1948    3]\n",
      " [   0    0    0    0    0    0    0    0  157   42 1946 1947    8   51\n",
      "  1948    3 1146]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0   11 1147]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    11 1147    6]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0   11\n",
      "  1147    6 1949]]\n"
     ]
    }
   ],
   "source": [
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen = max_sequence_len, padding = \"pre\"))\n",
    "print(input_sequences[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "xs = input_sequences[:,:-1]\n",
    "labels = input_sequences[:,-1]\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes = total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 10, input_length = max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(12, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
